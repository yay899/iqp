@inproceedings{10.1145/3328519.3329126,
    author = {Redyuk, Sergey and Schelter, Sebastian and Rukat, Tammo and Markl, Volker and Biessmann, Felix},
    title = {Learning to Validate the Predictions of Black Box Machine Learning Models on Unseen Data},
    year = {2019},
    isbn = {9781450367912},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3328519.3329126},
    doi = {10.1145/3328519.3329126},
    booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
    articleno = {Article 4},
    numpages = {4},
    location = {Amsterdam, Netherlands},
    series = {HILDA'19},
    abstract = {When end users apply a machine learning (ML) model on new unlabeled data, it is difficult for them to decide whether they can trust its predictions. Errors or shifts in the target data can lead to hard-to-detect drops in the predictive quality of the model. We therefore propose an approach to assist non-ML experts working with pretrained ML models. Our approach estimates the change in prediction performance of a model on unseen target data. It does not require explicit distributional assumptions on the dataset shift between the training and target data. Instead, a domain expert can declaratively specify typical cases of dataset shift that she expects to observe in real-world data. Based on this information, we learn a performance predictor for pretrained black box models, which can be combined with the model, and automatically warns end users in case of unexpected performance drops. We demonstrate the effectiveness of our approach on two models -- logistic regression and a neural network, applied to several real-world datasets.},
    annotate = {This regards the difficulty of validating the predictions of a machine learning (ML) models when errors in the input data induce unwanted and unknown changes in the model's weights. This is difficult due to it being legitimately hard to confirm if predictions are correct or not until whatever they predict has come to pass, and frequently this involves making unrealistic assumptions on the nature of possible dataset shifts. Emphasis is placed on the lack of availability of validation tools for use by non-ML experts. Unfortunately, the article only really goes into detecting problems, and states that the user should contact an ML expert in the event of degredation in the predictive model. That solution that is proposed seems interesting.}
}

@inproceedings{10.1145/2858036.2858529,
    author = {Krause, Josua and Perer, Adam and Ng, Kenney},
    title = {Interacting with Predictions: Visual Inspection of Black-Box Machine Learning Models},
    year = {2016},
    isbn = {9781450333627},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2858036.2858529},
    doi = {10.1145/2858036.2858529},
    booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
    pages = {5686--5697},
    numpages = {12},
    keywords = {predictive modeling, interactive machine learning, partial dependence},
    location = {San Jose, California, USA},
    series = {CHI '16},
    abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these naive estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why specific datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records.},
    annotate = {Details Prospector, a tool for visualizing ML models. The article uses a diabetes research, and has a focus on medical applications.}
}

@article{siau2018building,
    title = {Building trust in artificial intelligence, machine learning, and robotics},
    author = {Siau, Keng and Wang, Weiyu},
    journal = {Cutter Business Technology Journal},
    volume = {31},
    number = {2},
    pages = {47--53},
    year = {2018},
    abstract = {In this article, we look at trust in artificial intelligence, machine learning (ML), and robotics. We first review the concept of trust in AI and examine how trust in AI may be different from trust in other technologies. We then discuss the differences between interpersonal trust and trust in technology and suggest factors that are crucial in building initial trust and developing continuous trust in artificial intelligence.},
    annotate = {Formally defines trust, and then goes into the various factors that contribute to both initial trust and the process of building trust. Seems to suggest that initial trust doesn't have a whole lot to do with the actual functionality of the AI (just various things that affect perception.) Kinda basic, and doesn't focus too hard on ML, but still useful. It focuses way too hard on stereotypical AI things, even going as far to say "AI will continue to enhance its capability and infiltrate more domains." Consider replacing this.}
}

@misc{bbc2016rroai,
    author = {Nogrady, Bianca},
    title = {The real risks of artificial intelligence --- {BBC}},
    year = 2016,
    url = {https://www.bbc.com/future/article/20161110-the-real-risks-of-artificial-intelligence},
    eprint = {https://www.bbc.com/future/article/20161110-the-real-risks-of-artificial-intelligence},
    note = {[Online; accessed 28-Febuary-2020]},
    annotate = {Talks about how the fears regarding artificial intelligence don't really line up with the actual risks of artificial intelligence. Basically everyone is afraid of an AI uprising, and no one is afraid of how much trust we put into systems that are incredibly difficult to verify (IE, AI is a black box). Specifically addresses machine learning, and occasionally conflates it with procedural AI. Points out that AI is already used significantly.}
}

@misc{emerj2019roawrtiwwa,
    author = {Faggella, Daniel},
    title = {Risks of AI -- What Researchers Think is Worth Worrying About --- {EMERJ}},
    year = 2019,
    url = {https://emerj.com/ai-market-research/artificial-intelligence-risk/},
    eprint = {https://emerj.com/ai-market-research/artificial-intelligence-risk/},
    note = {[Online; accessed 28-Febuary-2020]},
    annotate = {Talks about more professional concerns of machine learning. Points out that AI will do nothing to alleviate current socioeconomic problems, and that large scale societal changes would be required for that. Also pointed out that the impact it has on automation will accelerate pollution, resource consumption, and financial inequality.}
}

@article{doi:10.1191/1478088706qp063oa,
    author = {Braun, Virginia and Clarke, Victoria},
    title = {Using thematic analysis in psychology},
    journal = {Qualitative Research in Psychology},
    volume = {3},
    number = {2},
    pages = {77-101},
    year  = {2006},
    publisher = {Routledge},
    doi = {10.1191/1478088706qp063oa},
    URL = {https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
    eprint = {https://www.tandfonline.com/doi/pdf/10.1191/1478088706qp063oa},
    abstract = {Thematic analysis is a poorly demarcated, rarely-acknowledged, yet widely-used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically-flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.},
    annotate = {Gives a tutorial on thematic analysis, and explains how it stands as its own method of qualitative analysis rather than a tool for qualitative analysis. Something doesn't have to be literally the most prevelant thing for it to be a theme, and there is no hard cutoff between what is and isn't a theme. Latent analysis focuses on the meaning behind data while semantic focuses on the contents of the data. (Read the actual guide on performing this later)}
}

@article{Corbin1990,
    author = {Corbin, Juliet M. and Strauss, Anselm},
    title = {Grounded theory research: Procedures, canons, and evaluative criteria},
    journal = {Qualitative Sociology},
    year = {1990},
    volume = {13},
    number = {1},
    pages = {3-21},
    abstract = {Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done for grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evaluative criteria.},
    issn = {1573-7837},
    doi = {10.1007/BF00988593},
    url = {https://doi.org/10.1007/BF00988593},
    annotate = {Grounded theory regards the sorting of data by rigid concepts that once defined can be identified in members of the data set in order to figure out how these concepts effect the phonomenon in question. It seems to be more focused on incidents and events rather than subjects, and intensity of events is important.}
}

@article{jpsp2002sadfsg,
    author = {Kenny, D. A. and Manetti, L. and Pierro, A. and Livi, S. and Kashy, D. A.},
    title = {The statistical analysis of data from small groups},
    journal = {Journal of Personality and Social Psychology},
    year = {2002},
    volume = {83},
    number = {1},
    pages = {126-137},
    issn = {0022-3514},
    doi = {10.1037/0022-3514.83.1.126},
    url = {https://doi.org/10.1037/0022-3514.83.1.126},
    abstract = {The authors elaborate the complications and the opportunities inherent in the statistical analysis of small-group data. They begin by discussing nonindependence of group members' scores and then consider standard methods for the analysis of small-group data and determine that these methods do not take into account this nonindependence. A new method is proposed that uses multilevel modeling and allows for negative nonindependence and mutual influence. Finally, the complications of interactions, different group sizes, and differential effects are considered. The authors strongly urge that the analysis model of data from small-group studies should mirror the psychological processes that generate those data.},
    annotate = {Regards data about small groups rather than individuals. Read the rest later, because it's not really relevant.}
}

@inproceedings {8631448, 
    author = {L. H. {Gilpin} and D. {Bau} and B. Z. {Yuan} and A. {Bajwa} and M. {Specter} and L. {Kagal}}, 
    booktitle = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)}, 
    title = {Explaining Explanations: An Overview of Interpretability of Machine Learning}, 
    year = {2018}, 
    volume = {}, 
    number = {}, 
    pages = {80-89}, 
    abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.}, 
    keywords = {artificial intelligence;data analysis;learning (artificial intelligence);neural nets;complex machines;algorithmic fairness;training data;suggested future research directions;explanatory artificial intelligence;explaining explanations;machine learning;XAI;potential bias-problems;Artificial intelligence;Computational modeling;Decision trees;Biological neural networks;Taxonomy;Complexity theory;Machine learning theories;Models and systems;Deep learning and deep analytics;Fairness and transparency in data science}, 
    doi = {10.1109/DSAA.2018.00018}, 
    ISSN = {null}, 
    month = {Oct},
    annotate = {Interpretability regards analyzing how a neural net made a specific decision. Explainable models are interpretable, but the inverse might not be true.Explainability regards whether the model provides insight into why it made a certain decision, and provides evidence to back up the decision. An explanation can be both complete and interpretable; the first is if it provides a complete understanding of the system and the second is if a human can actually understand the explanation. The two are usually at odds. Makes a point toward lack of completeness in explanations being unethical when the limitations are not interpretable.}
}

@article {fdaregproposal,
    author = {},
    title = {Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning {(AI/ML)}-Based Software as a Medical Device (SaMD)},
    year = {2019},
    annotate = {(Need more information in citation)}
}

@inproceedings{caruana2015intelligible,
    title = {Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},
    author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
    booktitle = {Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
    pages = {1721--1730},
    year = {2015},
    abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
    annotate = {The asthma thing.}
}

@misc{introtoauc,
    author = {Narkhede, Sarang},
    title = {Risks of AI -- What Researchers Think is Worth Worrying About --- {Towards Data Science}},
    year = 2018,
    url = {https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5},
    eprint = {https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5},
    note = {[Online; accessed 8-March-2020]},
    annotate = {Brief explanation of area under the curve (AUC) within the context of machine learning as a measure of performance. Also defines sensitivity and selectivity.}
}

@article{7912315,
    author = {M. {Chen} and Y. {Hao} and K. {Hwang} and L. {Wang} and L. {Wang}},
    journal = {IEEE Access},
    title = {Disease Prediction by Machine Learning Over Big Data From Healthcare Communities},
    year = {2017},
    volume = {5},
    number = {},
    pages = {8869-8879},
    abstract = {With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.},
    keywords = {Big Data;health care;learning (artificial intelligence);neural nets;disease prediction;healthcare communities;machine learning algorithms;real-life hospital data;convolutional neural network-based multimodal disease risk prediction algorithm;CNN-based multimodal disease risk prediction algorithm;medical big data analytics;Diseases;Hospitals;Prediction algorithms;Machine learning algorithms;Big Data;Data models;Big data analytics;machine learning;healthcare},
    doi = {10.1109/ACCESS.2017.2694446},
    ISSN = {2169-3536},
    month = {},
    annotate = {Details an algorithm for identifying high risk patients. Trained on data from central China due to there being very little work done there.}
}

@article{Ham2017,
    author = {Ham, Nathaniel
    and Dirin, Amir
    and Laine, Teemu H.},
    title = {Machine learning and dynamic user interfaces in a context aware nurse application environment},
    journal = {Journal of Ambient Intelligence and Humanized Computing},
    year = {2017},
    volume = {8},
    number = {2},
    pages = {259-271},
    abstract = {The increasing usage of smartphones in daily life has received considerable attention in academic and industry driven research to be utilized in the health sector. There has been development of a variety of health-related smartphone applications. Currently, however, there are few to none applications based on nurses' historical or behavioral preferences. Mobile application development for the health care sector requires extensive attention to security, reliability, and accuracy. In nursing applications, the users are often required to navigate in hospital environments, select patients to support, read the patient history and set action points to assist the patient during their shift. Finally, they have to report their performance on patient related activities and other relevant information before they leave for the day. In a working day, a nurse often visits different locations such as the patient's room, different laboratories, and offices for filling reports. There is still a limited capability to access context relevant information on a smartphone with minimal recourse such as Wi-Fi triangulation. The Wi-Fi triangulation signals fluctuate significantly for indoor location positioning. Therefore, providing relevant location based services to a mobile subscriber has become challenging. This paper addresses this gap by applying machine learning and behavior analysis to anticipate the potential location of the nurse and provide the required services. The application concept was already presented at the IMCOM 2015 conference. This paper focuses on the process to ascertain a user's context, the process of analyzing and predicting user behavior, and finally, the process of displaying the information through a dynamically generated UI.},
    issn = {1868-5145},
    doi = {10.1007/s12652-016-0384-1},
    url = {https://doi.org/10.1007/s12652-016-0384-1},
    annotate = {About using machine learning to streamline smartphone applications for nurses. Aims to aid nurse work by providing user specific UIs that match whatever the user is doing at any given moment.}
}

@article{doi:10.1111/j.1365-2648.2009.05082.x,
	author = {Van Bogaert, Peter and Meulemans, Herman and Clarke, Sean and Vermeyen, Karel and Van de Heyning, Paul},
	title = {Hospital nurse practice environment, burnout, job outcomes and quality of care: test of a structural equation model},
	journal = {Journal of Advanced Nursing},
	volume = {65},
	number = {10},
	pages = {2175-2185},
	keywords = {burnout, job satisfaction, nurse management, nurse practice environment, nurse retention, quality of care, structural equation modelling},
	doi = {10.1111/j.1365-2648.2009.05082.x},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2648.2009.05082.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2648.2009.05082.x},
	abstract = {Abstract Title. Hospital nurse practice environment, burnout, job outcomes and quality of care: test of a structural equation model. Aim. The aim of the study was to investigate relationships between nurse practice environment, burnout, job outcomes and nurse-assessed quality of care. Background. A growing line of work confirms that, in countries with distinctly different healthcare systems, nurses report similar shortcomings in their work environments and the quality of care in hospitals. Neither the specific work environment factors most involved in dissatisfaction, burnout and other negative job outcomes, and patient outcomes, nor the mechanisms tying nurse job outcomes to quality of care are well understood. Method. A Nurse Practice Environment and Outcome causal structure involving pathways between practice environment dimensions and outcome variables with components of burnout in a mediating position was developed. Survey data from 401 staff nurses across 31 units in two hospitals (including the Revised Nursing Work Index, the Maslach Burnout Inventory, and job outcome and nurse-assessed quality of care variables) were used to test this model using structural equation modelling techniques. The data were collected from December 2006 to January 2007. Results. Goodness of fit statistics confirmed an improved model with burnout dimensions in mediating positions between nurse practice environment dimensions and both job outcomes and nurse-assessed quality of care, explaining 20\% and 46\% of variation in these two indicators, respectively. Conclusion. These findings suggest that hospital organizational properties, including nurse--physician relations, are related to quality of care assessments, and to the outcomes of job satisfaction and turnover intentions, with burnout dimensions appearing to play mediating roles. Additionally, a direct relationship between assessments of care quality and management at the unit level was observed.},
	year = {2009},
    annotate = {Nurses overworked.}
}

@Article{PintodosSantos2019,
    author = {Pinto dos Santos, D. and Giese, D. and Brodehl, S. and Chon, S. H. and Staab, W. and Kleinert, R. and Maintz, D. and Bae{\ss}ler, B.},
    title = {Medical students' attitude towards artificial intelligence: a multicentre survey},
    journal = {European Radiology},
    year = {2019},
    volume = {29},
    number = {4},
    pages = {1640-1646},
    abstract = {To assess undergraduate medical students' attitudes towards artificial intelligence (AI) in radiology and medicine.},
    issn = {1432-1084},
    doi = {10.1007/s00330-018-5601-1},
    url = {https://doi.org/10.1007/s00330-018-5601-1},
    annotate = {Cited by Vayena2018 to support statistics in intro.}
}



@article{Vayena2018,
    author = {Vayena, Effy and Blasimme, Alessandro and Cohen, I. Glenn},
    title = {Machine learning in medicine: Addressing ethical challenges},
    journal = {PLoS medicine},
    year = {2018},
    month = {Nov},
    day = {06},
    publisher = {Public Library of Science},
    volume = {15},
    number = {11},
    pages = {e1002689-e1002689},
    keywords = {Attitude of Health Personnel, Attitude to Computers, Computer Security/*ethics, Confidentiality/*ethics, Data Mining/*ethics, Delivery of Health Care/ethics, Humans, Machine Learning/*ethics, *Medical Records, Public Opinion, Self Care/ethics, Trust},
    abstract = {Effy Vayena and colleagues argue that machine learning in medicine must offer data protection, algorithmic transparency, and accountability to earn the trust of patients and clinicians.},
    note = {30399149[pmid]},
    note = {PMC6219763[pmcid]},
    note = {PMEDICINE-D-18-03354[PII]},
    issn = {1549-1676},
    doi = {10.1371/journal.pmed.1002689},
    url = {https://pubmed.ncbi.nlm.nih.gov/30399149},
    annotate = {Has survey data regarding medical ML, and has a graphic that broadly outlines the various concerns. Points out that different places have reached different concensuses regarding sourcing data. It does this by showing the difference between GDPR and HIPPA. Covers how current regulation doesn't really work for ML. Honestly it's basically the exact same thing we're trying to do.}
}

@ARTICLE{6868201,
    author = {M. {Mozaffari-Kermani} and S. {Sur-Kolay} and A. {Raghunathan} and N. K. {Jha}},
    journal = {IEEE Journal of Biomedical and Health Informatics},
    title = {Systematic Poisoning Attacks on and Defenses for Machine Learning in Healthcare},
    year = {2015},
    volume = {19},
    number = {6},
    pages = {1893-1905},
    abstract = {Machine learning is being used in a wide range of application domains to discover patterns in large datasets. Increasingly, the results of machine learning drive critical decisions in applications related to healthcare and biomedicine. Such health-related applications are often sensitive, and thus, any security breach would be catastrophic. Naturally, the integrity of the results computed by machine learning is of great importance. Recent research has shown that some machine-learning algorithms can be compromised by augmenting their training datasets with malicious data, leading to a new class of attacks called poisoning attacks. Hindrance of a diagnosis may have life-threatening consequences and could cause distrust. On the other hand, not only may a false diagnosis prompt users to distrust the machine-learning algorithm and even abandon the entire system but also such a false positive classification may cause patient distress. In this paper, we present a systematic, algorithm-independent approach for mounting poisoning attacks across a wide range of machine-learning algorithms and healthcare datasets. The proposed attack procedure generates input data, which, when added to the training set, can either cause the results of machine learning to have targeted errors (e.g., increase the likelihood of classification into a specific class), or simply introduce arbitrary errors (incorrect classification). These attacks may be applied to both fixed and evolving datasets. They can be applied even when only statistics of the training dataset are available or, in some cases, even without access to the training dataset, although at a lower efficacy. We establish the effectiveness of the proposed attacks using a suite of six machine-learning algorithms and five healthcare datasets. Finally, we present countermeasures against the proposed generic attacks that are based on tracking and detecting deviations in various accuracy metrics, and benchmark their effectiveness.},
    keywords = {health care;learning (artificial intelligence);medical computing;pattern classification;security of data;systematic poisoning attacks;healthcare;application domains;critical decisions;biomedicine;health-related applications;security breach;machine-learning algorithms;training datasets;malicious data;life-threatening consequences;false diagnosis prompt users;false positive classification;patient distress;targeted errors;arbitrary errors;Machine learning algorithms;Training;Malware;Security;Healthcare;machine learning;poisoning attacks;security;Algorithms;Computer Security;Databases, Factual;Humans;Machine Learning;Medical Informatics;Models, Theoretical;Neoplasms},
    doi = {10.1109/JBHI.2014.2344095},
    ISSN = {2168-2208},
    month = {Nov},
    annotate = {Posioning attacks are bad and very hard to detect when you can't examine your data on account of it being sensitive and private information. The article details methods of performing poisoning attacks, and vaguely suggests some ways of defending from them.}
}

@Article{Chen2017,
    author = {Chen, Jonathan H. and Asch, Steven M.},
    title = {Machine Learning and Prediction in Medicine - Beyond the Peak of Inflated Expectations},
    journal = {The New England journal of medicine},
    year = {2017},
    month = {Jun},
    day = {29},
    volume = {376},
    number = {26},
    pages = {2507-2509},
    keywords = {Algorithms},
    keywords = {*Machine Learning},
    keywords = {Probability},
    keywords = {Prognosis},
    keywords = {Risk Assessment/methods},
    note = {28657867[pmid]},
    note = {PMC5953825[pmcid]},
    issn = {1533-4406},
    doi = {10.1056/NEJMp1702071},
    url = {https://pubmed.ncbi.nlm.nih.gov/28657867},
    annotate = {Points out that sources of data were originally intended for use for billing and clinical care; this introduces sources of bias that are eliminated by traditional non-ML methods of data analysis. Sourcing data from a specific year is essentially a single data point, and sourcing large data over an extended period of time is only effective if the future will resemble the past. Brings up chaos theory and how seemingly inconsequential variations (small enough to be rounding errors) in starting conditions can have vastly different results. Additionally, suggests incorperating more data streams such as browsing history. On top of this, even accurate predictions aren't always actionable (or reducing associated variables such as number of consultations would be directly harmful).}
}

@article{10.1001/jama.2017.18391,
    author = {Beam, Andrew L. and Kohane, Isaac S.},
    title = {{Big Data and Machine Learning in Health Care}},
    journal = {JAMA},
    volume = {319},
    number = {13},
    pages = {1317-1318},
    year = {2018},
    month = {04},
    abstract = {{Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non-machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors.It is no surprise then that medicine is awash with claims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians. Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care.}},
    issn = {0098-7484},
    doi = {10.1001/jama.2017.18391},
    url = {https://doi.org/10.1001/jama.2017.18391},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2675024/jama\_beam\_2018\_vp\_170174.pdf},
    annotate = {Points out that there are some pretty big similarities between machine learning and standard statistical models that can be used to "demystify" machine learning for medical professionals. There are also cases where something common could be clasified as machine learning when you define machine learning as rules learned purely from data, and this is used to make a point that there is a spectrum between human guided and pure machine learning methods of data analysis.}
}

@Article{Gianfrancesco2018,
    author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and Schmajuk, Gabriela},
    title = {Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data},
    journal = {JAMA internal medicine},
    year = {2018},
    month = {Nov},
    day = {01},
    volume = {178},
    number = {11},
    pages = {1544-1547},
    keywords = {*Algorithms},
    keywords = {*Electronic Health Records},
    keywords = {Healthcare Disparities},
    keywords = {Humans},
    keywords = {*Machine Learning},
    keywords = {Socioeconomic Factors},
    abstract = {A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning-based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.},
    note = {30128552[pmid]},
    note = {PMC6347576[pmcid]},
    note = {2697394[PII]},
    issn = {2168-6114},
    doi = {10.1001/jamainternmed.2018.3763},
    url = {https://pubmed.ncbi.nlm.nih.gov/30128552},
    annotate = {}
}

@misc{facebookbreach2019,
    author = {Murphy, Margi},
    title = {Millions of Facebook user records exposed in data breach --- {The Telegraph}},
    year = 2019,
    url = {https://www.telegraph.co.uk/technology/2019/04/03/millions-facebook-user-records-exposed-data-breach/},
    eprint = {https://www.telegraph.co.uk/technology/2019/04/03/millions-facebook-user-records-exposed-data-breach/},
    note = {[Online; accessed 5-April-2020]},
    annotate = {Reports on plain-text password storage.}
}

@misc{cnet2019databreaches,
    author = {Hodge, Rae},
    title = {2019 Data Breach Hall of Shame: These were the biggest data breaches of the year --- {cnet}},
    year = 2019,
    url = {https://www.cnet.com/news/2019-data-breach-hall-of-shame-these-were-the-biggest-data-breaches-of-the-year/},
    eprint = {https://www.cnet.com/news/2019-data-breach-hall-of-shame-these-were-the-biggest-data-breaches-of-the-year/},
    note = {[Online; accessed 5-April-2020]},
    annotate = {Statistics on frequency of data breaches and how much it has increased over the past year. Highlights a bunch of very bad data breaches.}
}

@article {Shameer1156,
	author = {Shameer, Khader and Johnson, Kipp W and Glicksberg, Benjamin S and Dudley, Joel T and Sengupta, Partho P},
	title = {Machine learning in cardiovascular medicine: are we there yet?},
	volume = {104},
	number = {14},
	pages = {1156--1164},
	year = {2018},
	doi = {10.1136/heartjnl-2017-311198},
	publisher = {BMJ Publishing Group Ltd},
	abstract = {Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.},
	issn = {1355-6037},
	URL = {https://heart.bmj.com/content/104/14/1156},
	eprint = {https://heart.bmj.com/content/104/14/1156.full.pdf},
	journal = {Heart},
    annotate = {Analysis of role machine learning is starting to take in cardiovascular patient care. Puts an emphasis on how much data is being gathered, and how it's far too much data for standard statistical methods. States that machine learning is a complement to standard statistical analysis and applies in different situations, providing a more robust prediction. Expects reinforcement learning to see more use than other forms of machine learning. Points out that predictive models trained in one hospital should not be expected to work in another. Acknowledges that missing data creates issues, but claims it creates more issues in statistical analysis than it does in machine learning. Another issue is that something common within dead patients is not the cause of their death, but rather something prescribed to treat an underlying issue with a high mortality rate regardless.}
}

@article{Shah2018,
    author = {Shah, Nilay and Steyerberg, Ewout and Kent, David},
    year = {2018},
    month = {05},
    pages = {},
    title = {Big Data and Predictive Analytics: Recalibrating Expectations},
    volume = {320},
    journal = {JAMA},
    doi = {10.1001/jama.2018.5602},
    abstract = {With the routine use of electronic health records (EHRs) in hospitals, health systems, and physician practices, there has been rapid growth in the availability of health care data over the last decade. In addition to the structured data in EHRs, new methods such as natural language processing can derive meaning from unstructured data, permitting the capture of substantial clinical information embedded in clinical notes. Furthermore, the growth in the availability of registries and claims data and the linkages between all these data sources have created a big data platform in health care, vast in both size and scope.},
    annotate = {}
}

@article{WU201868,
    title = {Behind the scenes: A medical natural language processing project},
    journal = {International Journal of Medical Informatics},
    volume = {112},
    pages = {68 - 73},
    year = {2018},
    issn = {1386-5056},
    doi = {https://doi.org/10.1016/j.ijmedinf.2017.12.003},
    url = {http://www.sciencedirect.com/science/article/pii/S138650561730446X},
    author = {Joy T. Wu and Franck Dernoncourt and Sebastian Gehrmann and Patrick D Tyler and Edward T Moseley and Eric T Carlson and David W Grant and Yeran Li and Jonathan Welt and Leo Anthony Celi},
    keywords = {Artificial intelligence in medicine, Natural language processing, Machine learning, Text analytics, Multidisciplinary teamwork, Cross-disciplinary research, Translational research},
    abstract = {Advancement of Artificial Intelligence (AI) capabilities in medicine can help address many pressing problems in healthcare. However, AI research endeavors in healthcare may not be clinically relevant, may have unrealistic expectations, or may not be explicit enough about their limitations. A diverse and well-functioning multidisciplinary team (MDT) can help identify appropriate and achievable AI research agendas in healthcare, and advance medical AI technologies by developing AI algorithms as well as addressing the shortage of appropriately labeled datasets for machine learning. In this paper, our team of engineers, clinicians and machine learning experts share their experience and lessons learned from their two-year-long collaboration on a natural language processing (NLP) research project. We highlight specific challenges encountered in cross-disciplinary teamwork, dataset creation for NLP research, and expectation setting for current medical AI technologies.},
    annotate = {}
}







@article{doi:10.1259/bjro.20190021,
    author = {Luo, Yi and Tseng, Huan-Hsin and Cui, Sunan and Wei, Lise and Ten Haken, Randall K. and El Naqa, Issam},
    title = {Balancing accuracy and interpretability of machine learning approaches for radiation treatment outcomes modeling},
    journal = {BJR|Open},
    volume = {1},
    number = {1},
    pages = {20190021},
    year = {2019},
    doi = {10.1259/bjro.20190021},
    URL = {https://doi.org/10.1259/bjro.20190021},
    eprint = {https://doi.org/10.1259/bjro.20190021},
    abstract = { Radiation outcomes prediction (ROP) plays an important role in personalized prescription and adaptive radiotherapy. A clinical decision may not only depend on an accurate radiation outcomes' prediction, but also needs to be made based on an informed understanding of the relationship among patients' characteristics, radiation response and treatment plans. As more patients' biophysical information become available, machine learning (ML) techniques will have a great potential for improving ROP. Creating explainable ML methods is an ultimate task for clinical practice but remains a challenging one. Towards complete explainability, the interpretability of ML approaches needs to be first explored. Hence, this review focuses on the application of ML techniques for clinical adoption in radiation oncology by balancing accuracy with interpretability of the predictive model of interest. An ML algorithm can be generally classified into an interpretable (IP) or non-interpretable (NIP) ("black box") technique. While the former may provide a clearer explanation to aid clinical decision-making, its prediction performance is generally outperformed by the latter. Therefore, great efforts and resources have been dedicated towards balancing the accuracy and the interpretability of ML approaches in ROP, but more still needs to be done. In this review, current progress to increase the accuracy for IP ML approaches is introduced, and major trends to improve the interpretability and alleviate the "black box" stigma of ML in radiation outcomes modeling are summarized. Efforts to integrate IP and NIP ML approaches to produce predictive models with higher accuracy and interpretability for ROP are also discussed. }
}

@article{ColeyRSC,
    author = {Coley, Connor},
    title = {A graph-convolutional neural network model for the prediction of chemical reactivity},
    journal = {The Royal Society of Chemistry},
    year = {2019}
}

@article{ford2016privacy,
    title = {Privacy and accountability in black-box medicine},
    author = {Ford, Roger Allan and Price, W and Nicholson, II},
    journal = {Mich. Telecomm. \& Tech. L. Rev.},
    volume = {23},
    pages = {1},
    year = {2016},
    publisher = {HeinOnline},
    annotate = {}
}

@article{reed2016responsibility,
    title = {Responsibility, Autonomy and Accountability: legal liability for machine learning},
    author = {Reed, Chris and Kennedy, Elizabeth and Silva, Sara},
    journal = {Queen Mary School of Law Legal Studies Research Paper},
    number = {243},
    year = {2016}
}

@misc{econ2020sense,
    author = {Hodge, Rae},
    title = {An antibody test for the novel coronavirus will soon be available --- {The Economist}},
    year = 2020,
    url = {https://www.economist.com/science-and-technology/2020/04/02/an-antibody-test-for-the-novel-coronavirus-will-soon-be-available},
    eprint = {https://www.economist.com/science-and-technology/2020/04/02/an-antibody-test-for-the-novel-coronavirus-will-soon-be-available},
    note = {[Online; accessed 18-April-2020]},
    annotate = {}
}

@article{samek2017explainable,
    title = {Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models},
    author = {Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert},
    journal = {arXiv preprint arXiv:1708.08296},
    year = {2017}
}

@article{goodman2017european,
    title = {European Union regulations on algorithmic decision-making and a “right to explanation”},
    author = {Goodman, Bryce and Flaxman, Seth},
    journal = {AI magazine},
    volume = {38},
    number = {3},
    pages = {50--57},
    year = {2017}
}

@article{wachter2017right,
    title = {Why a right to explanation of automated decision-making does not exist in the general data protection regulation},
    author = {Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
    journal = {International Data Privacy Law},
    volume = {7},
    number = {2},
    pages = {76--99},
    year = {2017},
    publisher = {Oxford University Press}
}

@article{ha2010doctor,
    title = {Doctor-patient communication: a review},
    author = {Ha, Jennifer Fong and Longnecker, Nancy},
    journal = {Ochsner Journal},
    volume = {10},
    number = {1},
    pages = {38--43},
    year = {2010},
    publisher = {Ochsner Journal}
}

@inproceedings{shin2018medical,
    title = {Medical image synthesis for data augmentation and anonymization using generative adversarial networks},
    author = {Shin, Hoo-Chang and Tenenholtz, Neil A and Rogers, Jameson K and Schwarz, Christopher G and Senjem, Matthew L and Gunter, Jeffrey L and Andriole, Katherine P and Michalski, Mark},
    booktitle = {International workshop on simulation and synthesis in medical imaging},
    pages = {1--11},
    year = {2018},
    organization = {Springer}
}

@article{shachor2020mixture,
    title = {A mixture of views network with applications to multi-view medical imaging},
    author = {Shachor, Yaniv and Greenspan, Hayit and Goldberger, Jacob},
    journal = {Neurocomputing},
    volume = {374},
    pages = {1--9},
    year = {2020},
    publisher = {Elsevier}
}