@inproceedings{10.1145/3328519.3329126,
    author = {Redyuk, Sergey and Schelter, Sebastian and Rukat, Tammo and Markl, Volker and Biessmann, Felix},
    title = {Learning to Validate the Predictions of Black Box Machine Learning Models on Unseen Data},
    year = {2019},
    isbn = {9781450367912},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3328519.3329126},
    doi = {10.1145/3328519.3329126},
    booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
    articleno = {Article 4},
    numpages = {4},
    location = {Amsterdam, Netherlands},
    series = {HILDA’19},
    abstract = {When end users apply a machine learning (ML) model on new unlabeled data, it is difficult for them to decide whether they can trust its predictions. Errors or shifts in the target data can lead to hard-to-detect drops in the predictive quality of the model. We therefore propose an approach to assist non-ML experts working with pretrained ML models. Our approach estimates the change in prediction performance of a model on unseen target data. It does not require explicit distributional assumptions on the dataset shift between the training and target data. Instead, a domain expert can declaratively specify typical cases of dataset shift that she expects to observe in real-world data. Based on this information, we learn a performance predictor for pretrained black box models, which can be combined with the model, and automatically warns end users in case of unexpected performance drops. We demonstrate the effectiveness of our approach on two models -- logistic regression and a neural network, applied to several real-world datasets.},
    annotate = {This regards the difficulty of validating the predictions of a machine learning (ML) models when errors in the input data induce unwanted and unknown changes in the model's weights. This is difficult due to it being legitimately hard to confirm if predictions are correct or not until whatever they predict has come to pass, and frequently this involves making unrealistic assumptions on the nature of possible dataset shifts. Emphasis is placed on the lack of availability of validation tools for use by non-ML experts. Unfortunately, the article only really goes into detecting problems, and states that the user should contact an ML expert in the event of degredation in the predictive model. That solution that is proposed seems interesting.}
}

@inproceedings{10.1145/2858036.2858529,
    author = {Krause, Josua and Perer, Adam and Ng, Kenney},
    title = {Interacting with Predictions: Visual Inspection of Black-Box Machine Learning Models},
    year = {2016},
    isbn = {9781450333627},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2858036.2858529},
    doi = {10.1145/2858036.2858529},
    booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
    pages = {5686–5697},
    numpages = {12},
    keywords = {predictive modeling, interactive machine learning, partial dependence},
    location = {San Jose, California, USA},
    series = {CHI ’16},
    abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these naive estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why specific datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records.},
    annotate = {Details Prospector, a tool for visualizing ML models. The article uses a diabetes research, and has a focus on medical applications.}
}

@article{siau2018building,
    title = {Building trust in artificial intelligence, machine learning, and robotics},
    author = {Siau, Keng and Wang, Weiyu},
    journal = {Cutter Business Technology Journal},
    volume = {31},
    number = {2},
    pages = {47--53},
    year = {2018},
    abstract = {In this article, we look at trust in artificial intelligence, machine learning (ML), and robotics. We first review the concept of trust in AI and examine how trust in AI may be different from trust in other technologies. We then discuss the differences between interpersonal trust and trust in technology and suggest factors that are crucial in building initial trust and developing continuous trust in artificial intelligence.},
    annotate = {Formally defines trust, and then goes into the various factors that contribute to both initial trust and the process of building trust. Seems to suggest that initial trust doesn't have a whole lot to do with the actual functionality of the AI (just various things that affect perception.) Kinda basic, and doesn't focus too hard on ML, but still useful. It focuses way too hard on stereotypical AI things, even going as far to say "AI will continue to enhance its capability and infiltrate more domains." Consider replacing this.}
}

@misc{bbc2016rroai,
    author = {Nogrady, Bianca},
    title = {The real risks of artificial intelligence --- {BBC}},
    year = 2016,
    url = {https://www.bbc.com/future/article/20161110-the-real-risks-of-artificial-intelligence},
    eprint = {https://www.bbc.com/future/article/20161110-the-real-risks-of-artificial-intelligence},
    note = {[Online; accessed 28-Febuary-2020]},
    annotate = {Talks about how the fears regarding artificial intelligence don't really line up with the actual risks of artificial intelligence. Basically everyone is afraid of an AI uprising, and no one is afraid of how much trust we put into systems that are incredibly difficult to verify (IE, AI is a black box). Specifically addresses machine learning, and occasionally conflates it with procedural AI. Points out that AI is already used significantly.}
}

@misc{emerj2019roawrtiwwa,
    author = {Faggella, Daniel},
    title = {Risks of AI -- What Researchers Think is Worth Worrying About --- {EMERJ}},
    year = 2019,
    url = {https://emerj.com/ai-market-research/artificial-intelligence-risk/},
    eprint = {https://emerj.com/ai-market-research/artificial-intelligence-risk/},
    note = {[Online; accessed 28-Febuary-2020]},
    annotate = {Talks about more professional concerns of machine learning. Points out that AI will do nothing to alleviate current socioeconomic problems, and that large scale societal changes would be required for that. Also pointed out that the impact it has on automation will accelerate pollution, resource consumption, and financial inequality.}
}

@article{doi:10.1191/1478088706qp063oa,
    author = {Braun, Virginia and Clarke, Victoria},
    title = {Using thematic analysis in psychology},
    journal = {Qualitative Research in Psychology},
    volume = {3},
    number = {2},
    pages = {77-101},
    year  = {2006},
    publisher = {Routledge},
    doi = {10.1191/1478088706qp063oa},
    URL = {https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
    eprint = {https://www.tandfonline.com/doi/pdf/10.1191/1478088706qp063oa},
    abstract = {Thematic analysis is a poorly demarcated, rarely-acknowledged, yet widely-used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically-flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.},
    annotate = {Gives a tutorial on thematic analysis, and explains how it stands as its own method of qualitative analysis rather than a tool for qualitative analysis. Something doesn't have to be literally the most prevelant thing for it to be a theme, and there is no hard cutoff between what is and isn't a theme. Latent analysis focuses on the meaning behind data while semantic focuses on the contents of the data. (Read the actual guide on performing this later)}
}

@article{Corbin1990,
    author = {Corbin, Juliet M. and Strauss, Anselm},
    title = {Grounded theory research: Procedures, canons, and evaluative criteria},
    journal = {Qualitative Sociology},
    year = {1990},
    volume = {13},
    number = {1},
    pages = {3-21},
    abstract = {Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done for grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evaluative criteria.},
    issn = {1573-7837},
    doi = {10.1007/BF00988593},
    url = {https://doi.org/10.1007/BF00988593},
    annotate = {Grounded theory regards the sorting of data by rigid concepts that once defined can be identified in members of the data set in order to figure out how these concepts effect the phonomenon in question. It seems to be more focused on incidents and events rather than subjects, and intensity of events is important.}
}

@article{jpsp2002sadfsg,
    author = {Kenny, D. A. and Manetti, L. and Pierro, A. and Livi, S. and Kashy, D. A.},
    title = {The statistical analysis of data from small groups},
    journal = {Journal of Personality and Social Psychology},
    year = {2002},
    volume = {83},
    number = {1},
    pages = {126-137},
    issn = {0022-3514},
    doi = {10.1037/0022-3514.83.1.126},
    url = {https://doi.org/10.1037/0022-3514.83.1.126},
    abstract = {The authors elaborate the complications and the opportunities inherent in the statistical analysis of small-group data. They begin by discussing nonindependence of group members' scores and then consider standard methods for the analysis of small-group data and determine that these methods do not take into account this nonindependence. A new method is proposed that uses multilevel modeling and allows for negative nonindependence and mutual influence. Finally, the complications of interactions, different group sizes, and differential effects are considered. The authors strongly urge that the analysis model of data from small-group studies should mirror the psychological processes that generate those data.},
    annotate = {Regards data about small groups rather than individuals. Read the rest later, because it's not really relevant.}
}

@inproceedings {8631448, 
    author = {L. H. {Gilpin} and D. {Bau} and B. Z. {Yuan} and A. {Bajwa} and M. {Specter} and L. {Kagal}}, 
    booktitle = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)}, 
    title = {Explaining Explanations: An Overview of Interpretability of Machine Learning}, 
    year = {2018}, 
    volume = {}, 
    number = {}, 
    pages = {80-89}, 
    abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.}, 
    keywords = {artificial intelligence;data analysis;learning (artificial intelligence);neural nets;complex machines;algorithmic fairness;training data;suggested future research directions;explanatory artificial intelligence;explaining explanations;machine learning;XAI;potential bias-problems;Artificial intelligence;Computational modeling;Decision trees;Biological neural networks;Taxonomy;Complexity theory;Machine learning theories;Models and systems;Deep learning and deep analytics;Fairness and transparency in data science}, 
    doi = {10.1109/DSAA.2018.00018}, 
    ISSN = {null}, 
    month = {Oct},
    annotate = {Interpretability regards analyzing how a neural net made a specific decision. Explainable models are interpretable, but the inverse might not be true.Explainability regards whether the model provides insight into why it made a certain decision, and provides evidence to back up the decision. An explanation can be both complete and interpretable; the first is if it provides a complete understanding of the system and the second is if a human can actually understand the explanation. The two are usually at odds. Makes a point toward lack of completeness in explanations being unethical when the limitations are not interpretable.}
}

@article {fdaregproposal,
    author = {},
    title = {Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD)},
    year = {2019},
    annotate = {(Need more information in citation)}
}

@inproceedings{caruana2015intelligible,
    title = {Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},
    author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
    booktitle = {Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
    pages = {1721--1730},
    year = {2015},
    abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
    annotate = {The asthma thing.}
}

@misc{introtoauc,
    author = {Narkhede, Sarang},
    title = {Risks of AI -- What Researchers Think is Worth Worrying About --- {Towards Data Science}},
    year = 2018,
    url = {https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5},
    eprint = {https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5},
    note = {[Online; accessed 8-March-2020]},
    annotate = {Brief explanation of area under the curve (AUC) within the context of machine learning as a measure of performance. Also defines sensitivity and selectivity.}
}

@article{7912315,
    author = {M. {Chen} and Y. {Hao} and K. {Hwang} and L. {Wang} and L. {Wang}},
    journal = {IEEE Access},
    title = {Disease Prediction by Machine Learning Over Big Data From Healthcare Communities},
    year = {2017},
    volume = {5},
    number = {},
    pages = {8869-8879},
    abstract = {With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.},
    keywords = {Big Data;health care;learning (artificial intelligence);neural nets;disease prediction;healthcare communities;machine learning algorithms;real-life hospital data;convolutional neural network-based multimodal disease risk prediction algorithm;CNN-based multimodal disease risk prediction algorithm;medical big data analytics;Diseases;Hospitals;Prediction algorithms;Machine learning algorithms;Big Data;Data models;Big data analytics;machine learning;healthcare},
    doi = {10.1109/ACCESS.2017.2694446},
    ISSN = {2169-3536},
    month = {},
    annotate = {Details an algorithm for identifying high risk patients. Trained on data from central China due to there being very little work done there.}
}

@article{Ham2017,
    author = {Ham, Nathaniel
    and Dirin, Amir
    and Laine, Teemu H.},
    title = {Machine learning and dynamic user interfaces in a context aware nurse application environment},
    journal = {Journal of Ambient Intelligence and Humanized Computing},
    year = {2017},
    volume = {8},
    number = {2},
    pages = {259-271},
    abstract = {The increasing usage of smartphones in daily life has received considerable attention in academic and industry driven research to be utilized in the health sector. There has been development of a variety of health-related smartphone applications. Currently, however, there are few to none applications based on nurses' historical or behavioral preferences. Mobile application development for the health care sector requires extensive attention to security, reliability, and accuracy. In nursing applications, the users are often required to navigate in hospital environments, select patients to support, read the patient history and set action points to assist the patient during their shift. Finally, they have to report their performance on patient related activities and other relevant information before they leave for the day. In a working day, a nurse often visits different locations such as the patient's room, different laboratories, and offices for filling reports. There is still a limited capability to access context relevant information on a smartphone with minimal recourse such as Wi-Fi triangulation. The Wi-Fi triangulation signals fluctuate significantly for indoor location positioning. Therefore, providing relevant location based services to a mobile subscriber has become challenging. This paper addresses this gap by applying machine learning and behavior analysis to anticipate the potential location of the nurse and provide the required services. The application concept was already presented at the IMCOM 2015 conference. This paper focuses on the process to ascertain a user's context, the process of analyzing and predicting user behavior, and finally, the process of displaying the information through a dynamically generated UI.},
    issn = {1868-5145},
    doi = {10.1007/s12652-016-0384-1},
    url = {https://doi.org/10.1007/s12652-016-0384-1},
    annotate = {About using machine learning to streamline smartphone applications for nurses. Aims to aid nurse work by providing user specific UIs that match whatever the user is doing at any given moment.}
}

@article{doi:10.1111/j.1365-2648.2009.05082.x,
	author = {Van Bogaert, Peter and Meulemans, Herman and Clarke, Sean and Vermeyen, Karel and Van de Heyning, Paul},
	title = {Hospital nurse practice environment, burnout, job outcomes and quality of care: test of a structural equation model},
	journal = {Journal of Advanced Nursing},
	volume = {65},
	number = {10},
	pages = {2175-2185},
	keywords = {burnout, job satisfaction, nurse management, nurse practice environment, nurse retention, quality of care, structural equation modelling},
	doi = {10.1111/j.1365-2648.2009.05082.x},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2648.2009.05082.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2648.2009.05082.x},
	abstract = {Abstract Title. Hospital nurse practice environment, burnout, job outcomes and quality of care: test of a structural equation model. Aim. The aim of the study was to investigate relationships between nurse practice environment, burnout, job outcomes and nurse-assessed quality of care. Background. A growing line of work confirms that, in countries with distinctly different healthcare systems, nurses report similar shortcomings in their work environments and the quality of care in hospitals. Neither the specific work environment factors most involved in dissatisfaction, burnout and other negative job outcomes, and patient outcomes, nor the mechanisms tying nurse job outcomes to quality of care are well understood. Method. A Nurse Practice Environment and Outcome causal structure involving pathways between practice environment dimensions and outcome variables with components of burnout in a mediating position was developed. Survey data from 401 staff nurses across 31 units in two hospitals (including the Revised Nursing Work Index, the Maslach Burnout Inventory, and job outcome and nurse-assessed quality of care variables) were used to test this model using structural equation modelling techniques. The data were collected from December 2006 to January 2007. Results. Goodness of fit statistics confirmed an improved model with burnout dimensions in mediating positions between nurse practice environment dimensions and both job outcomes and nurse-assessed quality of care, explaining 20\% and 46\% of variation in these two indicators, respectively. Conclusion. These findings suggest that hospital organizational properties, including nurse–physician relations, are related to quality of care assessments, and to the outcomes of job satisfaction and turnover intentions, with burnout dimensions appearing to play mediating roles. Additionally, a direct relationship between assessments of care quality and management at the unit level was observed.},
	year = {2009},
    annotate = {Nurses overworked.}
}

@article{Vayena2018,
    author = {Vayena, Effy and Blasimme, Alessandro and Cohen, I. Glenn},
    title = {Machine learning in medicine: Addressing ethical challenges},
    journal = {PLoS medicine},
    year = {2018},
    month = {Nov},
    day = {06},
    publisher = {Public Library of Science},
    volume = {15},
    number = {11},
    pages = {e1002689-e1002689},
    keywords = {Attitude of Health Personnel, Attitude to Computers, Computer Security/*ethics, Confidentiality/*ethics, Data Mining/*ethics, Delivery of Health Care/ethics, Humans, Machine Learning/*ethics, *Medical Records, Public Opinion, Self Care/ethics, Trust},
    abstract = {Effy Vayena and colleagues argue that machine learning in medicine must offer data protection, algorithmic transparency, and accountability to earn the trust of patients and clinicians.},
    note = {30399149[pmid]},
    note = {PMC6219763[pmcid]},
    note = {PMEDICINE-D-18-03354[PII]},
    issn = {1549-1676},
    doi = {10.1371/journal.pmed.1002689},
    url = {https://pubmed.ncbi.nlm.nih.gov/30399149},
    annotate = {Has survey data regarding medical ML, and has a graphic that broadly outlines the various concerns. Points out that different places have reached different concensuses regarding sourcing data. It does this by showing the difference between GDPR and HIPPA. Covers how current regulation doesn't really work for ML. Honestly it's basically the exact same thing we're trying to do.}
}

@ARTICLE{6868201,
    author = {M. {Mozaffari-Kermani} and S. {Sur-Kolay} and A. {Raghunathan} and N. K. {Jha}},
    journal = {IEEE Journal of Biomedical and Health Informatics},
    title = {Systematic Poisoning Attacks on and Defenses for Machine Learning in Healthcare},
    year = {2015},
    volume = {19},
    number = {6},
    pages = {1893-1905},
    abstract = {Machine learning is being used in a wide range of application domains to discover patterns in large datasets. Increasingly, the results of machine learning drive critical decisions in applications related to healthcare and biomedicine. Such health-related applications are often sensitive, and thus, any security breach would be catastrophic. Naturally, the integrity of the results computed by machine learning is of great importance. Recent research has shown that some machine-learning algorithms can be compromised by augmenting their training datasets with malicious data, leading to a new class of attacks called poisoning attacks. Hindrance of a diagnosis may have life-threatening consequences and could cause distrust. On the other hand, not only may a false diagnosis prompt users to distrust the machine-learning algorithm and even abandon the entire system but also such a false positive classification may cause patient distress. In this paper, we present a systematic, algorithm-independent approach for mounting poisoning attacks across a wide range of machine-learning algorithms and healthcare datasets. The proposed attack procedure generates input data, which, when added to the training set, can either cause the results of machine learning to have targeted errors (e.g., increase the likelihood of classification into a specific class), or simply introduce arbitrary errors (incorrect classification). These attacks may be applied to both fixed and evolving datasets. They can be applied even when only statistics of the training dataset are available or, in some cases, even without access to the training dataset, although at a lower efficacy. We establish the effectiveness of the proposed attacks using a suite of six machine-learning algorithms and five healthcare datasets. Finally, we present countermeasures against the proposed generic attacks that are based on tracking and detecting deviations in various accuracy metrics, and benchmark their effectiveness.},
    keywords = {health care;learning (artificial intelligence);medical computing;pattern classification;security of data;systematic poisoning attacks;healthcare;application domains;critical decisions;biomedicine;health-related applications;security breach;machine-learning algorithms;training datasets;malicious data;life-threatening consequences;false diagnosis prompt users;false positive classification;patient distress;targeted errors;arbitrary errors;Machine learning algorithms;Training;Malware;Security;Healthcare;machine learning;poisoning attacks;security;Algorithms;Computer Security;Databases, Factual;Humans;Machine Learning;Medical Informatics;Models, Theoretical;Neoplasms},
    doi = {10.1109/JBHI.2014.2344095},
    ISSN = {2168-2208},
    month = {Nov},
    annotate = {Posioning attacks are bad and very hard to detect when you can't examine your data on account of it being sensitive and private information. The article details methods of performing poisoning attacks, and vaguely suggests some ways of defending from them.}
}