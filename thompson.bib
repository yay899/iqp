@inproceedings{10.1145/3328519.3329126,
    author = {Redyuk, Sergey and Schelter, Sebastian and Rukat, Tammo and Markl, Volker and Biessmann, Felix},
    title = {Learning to Validate the Predictions of Black Box Machine Learning Models on Unseen Data},
    year = {2019},
    isbn = {9781450367912},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.ezpxy-web-p-u01.wpi.edu/10.1145/3328519.3329126},
    doi = {10.1145/3328519.3329126},
    booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
    articleno = {Article 4},
    numpages = {4},
    location = {Amsterdam, Netherlands},
    series = {HILDA’19},
    annotate = {This regards the difficulty of validating the predictions of a machine learning (ML) models when errors in the input data induce unwanted and unknown changes in the model's weights. This is difficult due to it being legitimately hard to confirm if predictions are correct or not until whatever they predict has come to pass, and frequently this involves making unrealistic assumptions on the nature of possible dataset shifts. Emphasis is placed on the lack of availability of validation tools for use by non-ML experts. Unfortunately, the article only really goes into detecting problems, and states that the user should contact an ML expert in the event of degredation in the predictive model. That solution that is proposed seems interesting.}
}

@inproceedings{10.1145/2858036.2858529,
    author = {Krause, Josua and Perer, Adam and Ng, Kenney},
    title = {Interacting with Predictions: Visual Inspection of Black-Box Machine Learning Models},
    year = {2016},
    isbn = {9781450333627},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.ezpxy-web-p-u01.wpi.edu/10.1145/2858036.2858529},
    doi = {10.1145/2858036.2858529},
    booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
    pages = {5686–5697},
    numpages = {12},
    keywords = {predictive modeling, interactive machine learning, partial dependence},
    location = {San Jose, California, USA},
    series = {CHI ’16},
    annotate = {Details Prospector, a tool for visualizing ML models. The article uses a diabetes research, and has a focus on medical applications.}
}

@article{siau2018building,
    title = {Building trust in artificial intelligence, machine learning, and robotics},
    author = {Siau, Keng and Wang, Weiyu},
    journal = {Cutter Business Technology Journal},
    volume = {31},
    number = {2},
    pages = {47--53},
    year = {2018},
    annotate = {Formally defines trust, and then goes into the various factors that contribute to both initial trust and the process of building trust. Seems to suggest that initial trust doesn't have a whole lot to do with the actual functionality of the AI (just various things that affect perception.) Kinda basic, and doesn't focus too hard on ML, but still useful. It focuses way too hard on stereotypical AI things, even going as far to say "AI will continue to enhance its capability and infiltrate more domains." Consider replacing this.}
}

  
