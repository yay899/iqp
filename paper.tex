\documentclass[]{article}

%opening
\title{Ethical Considerations of Artificial Intelligence via Neural Networks Applied to Medical Applications}
\author{
  Ternent, James\\
  \texttt{jwternent@wpi.edu}
  \and
  Thompson, Maximilian\\
  \texttt{mthompson2@wpi.edu}
}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Placeholder abstract
	\end{abstract}
	
	\section{Introduction}
		% This section will contain our research questions, and what we did to answer them (research, surveys, data analysis. Prefacing all of this will be the definition of some important terms that are used throughout the paper. Most of our research questions are outlined in another document.	
	
		Artificial intelligence (AI) and, more particularly, machine learning (ML) is becoming increasingly prevalent in medical fields. Though regulations are being discussed regarding AI as a whole, the regulations proposed by the FDA\cite{fdaregproposal} place a large emphasis on procedural AI which leaves the capabilities of ML models relatively less examined. Simply refusing to implement those restrictions would cause more harm than good, however. We wish to add to this discussion in the hopes of extending the ethical reasoning and guidance with regards to neural network implementations, so as to achieve more safety in applications of these devices and to greater understand the realm of applicability of these devices.

		% Change gets overused in this paragraph.
		Machine learning comprises a subset of AI that closely matches what most commonly comes to mind when AI is mentioned---both conversationally and within public culture.\textbf{[Citation needed]} The difference between procedural AI and ML is detailed in section \ref{introtoai}, but, in essence, procedural AI takes input data and outputs a response by following a strict set of rules or procedures, while ML takes input data and outputs a response determined by a set of facets of interest and weights that may change and shift in response to each input. Frequently ML models undergo a period of "training", during which weights may shift wildly, with the rate of change slowing down until it reaches a period of relatively little change.\textbf{[Citation needed]}

		We also provide an overview of ethical considerations within the context of medical devices in section \ref{ethics} before continuing to weigh the pros and cons of machine learning. We take survey of a number of nursing students to gauge their general opinion, understanding, and expectations of ML. We believe, due to how overworked nurses can be\cite{doi:10.1111/j.1365-2648.2009.05082.x} and how much of an impact ML has had and will have on the nursing population\cite{Ham2017}, that their viewpoints are and important part of the conversation which we wish to highlight. Finally, in section \ref{market}, we delve into the role both regulation and the free market can play in this developing field, and attempt to strike a balance between the two that we hope could create a safe environment for patients and practitioners alike without stifling the development of more refined machine learning and artificial intelligence.

		\subsection{Review of Terminology}
			% This part is definitely really rough, and needs to be expanded. It should potentially be moved to other situations. Probably to the intro to AI section.
			First however, we must define some terms used commonly in the ML field that may be misleading to those without a background in it. There are a number of terms used throughout the field of AI that may be misleading at first glance: "feature" refers to what amounts to an interesting property or value in the input data represented by a function explicitly defined by the developers of the AI. Within the context of ML, these functions are what is modified by changes in the weights of the model.\textbf{[Citation needed?]} Additionally, the term area under the curve (AUC) is used to express the quality of predictions derived from values referred to as sensitivity, the rate of positive predictions (true or false), and selectivity, the rate of negative predictions (true or false). It is important to note that the terms positive and negative apply to predictions, while true and false apply exclusively to the reality of the situation.\cite{introtoauc}

			Explainability of ML models is an important ongoing research field. An explanation of a model seeks to convey why a model made the decisions it did and why it outputted what it did, and explanations have two key attributes: completeness (how accurately the explanation conveys the workings of the ML model), and interpretability (how easy it is for humans to understand the explanation). The two are often at odds, and finding the solution to this problem is an important field of study\cite{8631448}---especially regarding the validation of ML model outputs.\cite{10.1145/3328519.3329126} Unfortunately, the term interpretability is not only applied to explanations, but also to the ML models themselves. When used to describe a model, the term refers to how easy it is to identify and understand the mechanisms that drive the machine. Essentially interpretability seeks to answer how the machine works, while explainability regards why the machine did what it did.\cite{8631448} More in depth definitions of these concepts are given in section \ref{introtoai}.

			% This paragraph should probably be moved.
			This topic also leads to a number of ethical considerations (further discussed in section \ref{ethics}) as pointed out by Gilpin \emph{et al.} who state that "[they] believe that it is fundamentally unethical to present a simplified description of a complex system in order to increase trust if the limitations of the simplified description cannot be understood by users". They point out that an explanation that meets that criteria are both misleading and dangerous.\cite{8631448} Though they do not mention it, we would like to add that though it may often be unintentional, this could easily be used to intentionally deceive. 
			
	\section{Basics of Artificial Intelligence}\label{introtoai}
		
		Artificial Intelligence (AI) in its basic form is a computer program that takes in an input or inputs and produces an output.  An example would be if a program took in how many hours it had been since someone washed their hands and determined whether they were due for washing their hands or not  The AI would have a threshold that would trigger a recommendation to shower.  For the purpose of this example, we will set it at two hours.  If it had been half an hour since someone washed their hands, it would not tell them to wash their hands, but if it had been five hours since they washed their hands, the AI would recommend washing their hands.  However, we know time is not the only factor in whether someone should wash their hands.  To better improve this theoretical AI, a second input could be added such as whether the person is about to touch or consume food.  In general, people should wash their hands if they are about to touch or consume food.  The updated logic in the AI is that it will recommend washing hands if the time since the last time someone washed their hands is greater than two hours or if they are about to touch or consume food. Only one condition has to be true to recommend a washing hands, but both could be true.  The more inputs or parameters that the AI takes in, the better it will get at giving a correct output as long as the researcher determines the correct thresholds for the AI.

		Eventually as the AI gets more complex, two problems present themselves.  First, not all problems that an AI tries to solve has a simple linear relation between their inputs and the output.  Second, the researcher may not know the exact relation between the inputs and the output.  This is where a subset of AI called Machine Learning (ML) comes into play.  A common type of  ML is called a Neural Network (NN) which more or less simulates a brain to a far lesser extent.  The way the brain works is it has many neurons that are connected together by axons and synapses. Axons are used to transport the electrical signals for the neurons. Synapses receive the signals at the end of the axons. Each synapse has a strength that determines how important the signal matters whether from another neuron or an input from the body. A neuron can have multiple inputs. The neuron sums all the signals with some mattering more than the others based on their synaptic strength. Once the neuron reaches a certain sum, it fires sending signals to other neurons or the body. The synaptic strengths can be changed over time based on whether the response generated by the neurons was correct for whatever situation. Over time, the synaptic strengths for a neuron are optimized to provide a correct response most of the time. Neural Networks work in a similar way. Each NN has nodes and weights. Nodes are like the neurons in the brain and weights are like the synaptic strength. The most basic NN has two layers. A layer of input nodes that takes the inputs in and passes them onto the next layer, the output layer. The hidden layer takes in the input, multiplies them by their weights, and sums them. An activation function maps the sum of the adjusted inputs to the desired output. The way the NN is trained through using data with known inputs and outputs. An algorithm known as error backpropagation is used to adjust the weights. In simple terms, it compares the output using the current weights, compares it to the desired output, and adjusts the weights until there is a little to no error.\cite{bishop2006pattern} The weights can be either increase or decrease based on the error backpropagation algorithm. Higher weights means an input is more important to determining the outcome, but lower weights means that whatever input is not as important. The weights allow the Neural Network to decide the features, what is important to the output. This is one of the things that makes NN’s so powerful. The example of washing hands can also be modeled with a NN. It would have two input nodes. One would take in the number of hours since someone had washed their hands and the other would take in whether someone was about to touch or consume food. The input nodes send the inputs to the output node where the inputs would be multiplied by their weights and summed. At first the weights would be randomly assigned or assigned to an estimate based off of previous knowledge of the researcher. An activation function would then be used to turn the sum into a yes or no of whether someone should wash their hands. Training data with known inputs and outputs would be used with error back propagation to adjust the weights until the error was eliminated. More inputs can also be added to improve the NN, but the real power of NN comes by adding a layer or layers between the input and output layers. Nodes can also be added to these layers. The more nodes the better. Once there is more than one layer, the connections between the nodes of separate layers can be chosen. The nodes of the previous layer do not have to connect to all the nodes of the next layer. More layers, nodes, and connections will make the NN better, but as the complexity increases, the time that it takes for the NN to produce an output increases. At a certain point of adding layers, nodes, and connections, the improvement of the success rate will be marginal in comparison with increase in time to run the NN. Neural Networks are used to model nonlinear problems linearly which can be very useful in the medical field.\cite{introtoneuralnets}
		
		Artificial Intelligence exists on a scale between human driven data analysis and machine driven analysis. Closer to humans doing the analysis of data is the example of the procedural AI for washing hands. Researchers define the rules for washing hands that make the decision of whether someone should wash their hands. These rules are hard coded into the AI. Somewhere in the middle of this scale is the Neural Network version of the hand washing AI. An algorithm determines the relationship between the parameters and the output, but the researchers chose two parameters which they felt were important. At the far end of the machine driven side, algorithms do most of the work. A perfect example of this is image recognition. The only human input is a researcher that annotates the images with the correct response. The algorithm figures out what makes a dog image different from a cat image for example with no human intervention. The algorithms needed are often called deep learning algorithms due to the multiple layers of the Neural Networks they create. Deep learning algorithms have the power to find the connection between vast amounts of data and the proper output, helping humans understand something that would be otherwise too overwhelming. However the more control that is given to algorithms, the less you can guarantee accuracy or fairness. They more or less become blackboxes. Although there are these risks, algorithms or machine learning can vastly help when it comes to medicine.\cite{10.1001/jama.2017.18391}
		
		Artificial Intelligence in medicine mainly focuses around a few disease types: cancer, nervous system diseases, and cardiovascular diseases. These diseases have hit humanity the hardest which is why they are the focus of researchers today. In each of these diseases, diagnosis imaging has been the focus of research. This is where machine learning thrive, finding the connection between symptoms, data from other non-invasive tests and what life altering disease a patient may have. A great example of the use of machine learning in medicine is in early stroke detection. A movement detection device was used to monitor a patient’s movements. The device would record both the normal movement of a person and the movement before a stroke started. By using machine learning, the device was able to tell when a person’s movement patterns differed significantly from their normal ones. AI has the power to save many lives in the medical field, but researchers must be careful to not cause any unnecessary risks to patients and the medical professionals using an AI.\cite{Jiang230}
		
	\section{Ethics of Medical Devices and ML}\label{ethics}
		% This section will go into the basic definition of ethics before going into more detail about how ethics apply to medical devices. There will also be some stuff on the legal and ethical considerations of using electronic health records as training data for learning AI.

		In their book Principles of Biomedical Ethics, Beauchamp and Childress outline four principles: autonomy, beneficence, nonmaleficence, and justice. Each principle is used to help medical professional make decisions on treating patients.

		The first principle autonomy refers to the ability to decide for oneself free from control of others and with sufficient level of understanding as to provide for meaningful choice. Also, the person deciding should have capacity to make the choice. Next, beneficence is the principle of weighing the risks against benefits in order to get the best result. This applies to not only the patient, but it also applies to society in general. The benefits to society should be considered when weighing the risks. Nonmaleficence can be summed up with \emph{primum non nocere}, first do no harm. A medical professional should do no harm. Finally, justice addresses the question of who receives healthcare resources as they are in scarce supply.\cite{lawrence2007four}

		Artificial Intelligence is primarily being developed as a diagnostic tool to help medical professionals. In order to develop these tools, researchers need access to large amounts of data such as Electronic Health Records. Machine learning algorithms use this data to define features. However, the Electronic Health Record (EHR) of a person is private information for use by doctors to treat said person. This brings into question is the ethics of the use of a patient’s Electronic Health Records.

		When using EHR, the first and most important principle, autonomy, should be thought of. Will patients allow the use of their EHR’s to develop AI’s that could save many people. Patients will need to be informed of what their EHR’s are being used for and what the risks are. The risk of using EHR’s should be weighed against the benefits. The main risk of using EHR’s is having patient information leaked and used against them. This risk can be minimized by removing identifiable information from the EHR’s before being used for research.

		The other part of Artificial Intelligence to consider is the deployment of it in the medical field. Patients that will have the option to have AI help with their treatment. They should know that an algorithm defined what was important in determining their future diagnosis and how much input a doctor has in the final diagnosis. Any risk that has appeared in testing needs to be communicated to the patients. They need to be informed to make a decision. The beneficence of the AI needs to be examined. Do the benefits justify the risks? At what point are the risks too great? These are questions researchers and health professionals need to ask themselves. One thing doctors live by is to do no harm. If they use an AI can they guarantee that the risk of harm is minimal. The final question of medical AI is the justice of them. Medical AI could become life saving tools, but the likelihood is that they will only be able to be used in developed countries. Less developed countries could miss out on them and fall further behind the world in health.

		Ethics are something that need to be considered especially with new unproven technology. Medical AI has the power to change the world, but they should not be released without considering the ethics behind them.

	\section{Expectations of ML in Medicine}
		% This section will go into how nurses and patients view medical AI, as well as how they expect it to function. This will make heavy use of gathered survey data and highlight both differences between the two groups, and just general areas that could cause some concern if they are not kept in mind while developing medical applications of machine learning.

		There are unrealistically high hopes for machine learning in medicine\cite{Chen2017,10.1001/jama.2017.18391}, and there are just as many unrealistic fears to complement them\cite{bbc2016rroai}. We chose to conduct our own survey of nursing students to gather opinions on general expectations of ML in medicine, as well as hopes and concerns. As a frequently understaffed and overworked profession\cite{doi:10.1111/j.1365-2648.2009.05082.x}, we believe that among healthcare professionals they stand to gain the most from advances in medical ML, and thus would provide valuable insight. Unfortunately, due to the ongoing COVID-19 pandemic at the time of writing, we felt that it would be inappropriate to administer such a survey. We believed that it may cause undue distress for participants given the extenuating circumstances most find themselves in.

		A number of surveys summarized by Vayena \emph{et al.} show a clear divide between public and professional opinion on medical ML: "63\% of the adult population [of the United Kingdom] is uncomfortable with allowing personal data to be used to improve healthcare and is unfavorable to artificial intelligence (AI) systems replacing doctors and nurses in tasks they usually perform." However, drawing from a German survey, they express that a large portion of medical students believe wholeheartedly that machine learning will improve medicine. Though they are not eager enough to embrace it with little caution, expressing that they are "skeptical that it will establish conclusive diagnoses."\cite{Vayena2018} This opinion is roughly mirrored by another United States survey cited by the authors of the article.\cite{PintodosSantos2019} These statistics illustrate a clear disconnect between public and professional opinions, that warrants further discussion.

		\subsection{Public Opinion}

			An article by Chen and Asch expresses how the public hopes for the predictive power of machine learning in medicine vastly outstrip the realistic potential for the emerging technology. The limitations of data sources (such as electronic health records) can introduce bias\cite{Gianfrancesco2018} due to the fact that they were only ever intended to be used (in their current state) for clinical care and billing. They propose adding additional data sources such as browser history to diversify and improve predictive power. However, they are quick to add that even that makes some glaring assumptions regarding how closely the future will resemble the present. It is incredibly unlikely that machine learning models will ever be able to predict the date of a catastrophe---just the risk of one occurring within a given period of time. Regardless, they are still hopeful for the future of machine learning, showing greater concern for the potential backlash when the predictive power of machine learning doesn't meet these expectations.\cite{Chen2017}

			Chen and Asch, however make an assumption regarding the availability of numerous unfettered sources of user information in an era where large scale data breaches have become almost commonplace with, as reported by cnet, 5,183 data breaches within 2019 alone.\cite{cnet2019databreaches} With bills such as the General Data Protection Regulation (GDPR) bringing well deserved attention to the storage and processing of personal data people are significantly more skeptical of what personal data is being collected and how it is being used.\cite{Vayena2018} While this does promote the ethical handling of personal data, it may create greater pressure on researchers to identify a concrete reason for including a data stream; this has the potential to introduce bias through the choice of which data streams are available to a given ML model and which are deemed unnecessary.\cite{Gianfrancesco2018}
			
			These developments raise important questions: how much loss of privacy are the potential improvements to medical decision making given by machine learning worth? How much privacy is the public willing to part with regardless of the value? Though promises may be made regarding maintaining privacy through safe storage of data, it is evident that data breaches have become more a question of when rather than a question of if.

		\subsection{Professional Opinion}

			Shameer \emph{et al} express hope regarding the future of machine learning and cardiology, while acknowledging that there are a number of issues that must be overcome. Additionally, they note that machine learning is more likely to be a complement to standard statistical analysis rather than a replacement.\cite{Shameer1156} This falls roughly in line with what is expressed in the article by Vayena \emph{et al}\cite{Vayena2018}, and the same trend can be seen in a number of articles.\cite{Shah2018,WU201868}

	\section{Benefits of Medical AI}
		% This section will go into what sort of benefits medical AI may be able to provide regardless of what people expect of it. It will bring up how it can lighten the workload of overworked healthcare professionals, and how it may lead to better treatment selection by pulling from data a human would not expect to be relevant.

		When discussing the potential benefits of machine learning for use in medical settings, it's easy to assume that use cases will be centered around aiding health care professionals in decision making and diagnosis or patients in seeking the correct medical care, but there is just as much potential for machine learning to alleviate some of the stress of day to day nursing work simply by streamlining existing systems in ways that match any given situation. In 2016, Ham \emph{et al.} proposed and developed a proof of concept application that provides contextual assistance using wi-fi based location data to aid a machine learning algorithm.\cite{Ham2017}

	\section{Risks of Medical AI}
		Within public media, perception of the risk of ML is often centered entirely around the threat of some kind of AI uprising.\cite{bbc2016rroai} In contrast, the largest threats that concern professionals are, shown by a survey conducted by Danial Faggella in 2019, the lack of explainability and verifiability in most machine learning models complicated enough to be of use in medicine, along with (though not strictly related to medicine) the potential socio-economic and environmental impact of increased use of machine learning.\cite{emerj2019roawrtiwwa}
			
		While skepticism regarding upcoming technology is conducive to the development of safe and reliable systems, misdirected skepticism is more likely to mask potential issues rather than call attention to them. This is a huge problem for issues such as a general lack of explainability---an issue that is greatly magnified by blind acceptance of the output of a machine learning model based solely on the fact that the machine learning model isn't actively attempting to overthrow humanity. 

		\subsection{Lack of Explainability in Sufficiently Complex Systems}
			A key issue lies in two important facets of machine learning: explainability of machine learning models and interpretability of their results. Luo \emph{et al.} define explainability as the ability to "summarize the reasons for the behavior of [machine learning] algorithms" and interpretability as the ability to "comprehend what a model did."\cite{doi:10.1259/bjro.20190021} Though the difference between the two is subtle, the distinction is still an important one to make; methods that may increase interpretability may not yield results regarding the overall explainability of the model. However, improvements to explainability increase interpretability by definition.

			More often than not machine learning models are a black box---a machine learning model with very little explainability---that takes in datasets and outputs values with some prescribed significance. It is difficult for non-machine learning experts understand the risks and potential failure points of such a model, and methods of addressing this lack of explainability are still being debated.\cite{10.1145/2858036.2858529,10.1145/3328519.3329126} This in conjunction with a typical person's initial trust placing little to no emphasis on the actual functionality of the model\cite{siau2018building} can make for sudden and unexpected tragedy within medical fields.
			
			For example a CEHC study in the mid 90's found a rule-based trained to incorrectly identified pneumonia patients with asthma as having lower risk due to the lower mortality rate following urgent movement to intensive care.\cite{caruana2015intelligible} If such an algorithm were used in a real hospital environment, it is unlikely that there would be enough resources or time to devote to interpreting the results to identify this. However, if the algorithm were explainable to the layman this issue would be much more readily identifiable. As Connor points out: "the architecture of deep learning models should inherently enable some degree of interpretability as has been done in natural language processing"

			Improving explainability of algorithms and interpretability of results isn't as simple as, for instance, writing a more in depth manual. Coley states in his 2019 article that "the architecture of deep learning models should inherently enable some degree of interpretability."\cite{ColeyRSC} The burden falls to researchers and developers to improve these important aspects of machine learning, and not those in charge of documenting.
			
			To make matters more complicated, however, it is important to consider who the audience is when determining if an algorithm is explainable. If an algorithm can theoretically be explained but requires a degree in the subject to understand, can it really be considered explainable for medical professionals and patients? We believe that those who's lives are impacted by usage of machine learning techniques are ethically entitled to at the very least a cursory understanding of the given algorithm and the information it has procured.

		\subsection{Difficulty of Validation}
			There is immense difficulty of validating the predictions (before acting on them) of a machine learning model. This creates further risk by obfuscating potential error behind a highly technical veil for non-machine learning experts. Moreover, unknown error introduced in input datasets is difficult to detect without access to the data itself, as the ML model will continue to output seemingly valid predictions\cite{10.1145/3328519.3329126} Error could be introduced through any number of innocuous vectors (differences in storage systems between health centers, errors digitizing physical records, decay of electronic records over time) or through more malignant ones (such as a targeted attack). The ethical delicacy of handling electronic health records\cite{ford2016privacy} can make impossible to directly examine new inputs for a source of error, and even if it were possible, the sheer magnitude of the task would be prohibitive. Redyuk \emph{et al.} proposes a method of detecting dataset shifts (without direct examination) that may mitigate this issue in the future\cite{10.1145/3328519.3329126}, but it still remains something important to discuss.

			Shifts in datasets are not the only thing that may cause unexpected error in outputs. Poor training, though ideally easier to identify simply due to training occurring well before live usage, is just as much of a risk. Unintentional biases may result from flawed training data\cite{7912315} or training data that has intentionally been poisoned or otherwise tampered with\cite{6868201}. In the case of healthcare, these issues may be incredibly difficult to detect once a machine learning model is being used in production---again due to the delicate nature of electronic health records. It may not be possible to verify that all training data is both intact and correct without crossing ethical or legal bounds.
			
			The consequences of both unforeseen shifts in inputs and poor training may prove discriminatory or even life threatening. Even given some virtually infinite workforce to attempt to validate all results of machine learning models, doing so would require a level of precognizance that is currently impossible to attain.

		\subsection{Diffusion of Liability}
			Though largely considering more concrete applications of machine learning (such as self driving cars), Reed {et al.}'s 2016 article on the legal responsibility and liability (with regards to English law) points out that there are several parties that could be considered at fault should harm come to a patient due to the complex nature of development and usage of machine learning models. Moreover they posit that it would be incredibly difficult to establish the fault of one party and significantly more expensive to establish the fault of several parties in unison.\cite{reed2016responsibility}

			The difficulty of proving liability has the potential to do significant harm. Should a defective algorithm result in patient harm, the blame might not even reach the manufacturers of the algorithm due to the difficulty of proving so---allowing said algorithm to see continued usage. Inversely, blame may falsely be pinned on the manufacturer giving a clinician an easy way to avoid being held accountable for medical malpractice. These issues are only aggravated by "black box" models making it more difficult to validate the output of the algorithm and assign appropriate blame.


		%\subsection{Maintenance}
			%The maintenance of a medical artificial intelligence (AI) is an important aspect that needs to be thoroughly examined. Without the implementation of maintenance of the medical AI, the performance could be degraded as the AI is trained on new data. As time goes along, it would become more likely for some form of AI malpractice to happen.
		
			%Maintenance could be manual or automated. However, manual maintenance runs into the problem of the extremely high workload, which makes it infeasible for humans to labor at. The only option is automated maintenance, but there are a variety of questions to consider to ensure a safe AI.
		
			%The first of the questions is how will quality assurance (QA) be performed with an AI continuously learning. The initial proposed framework for QA is the AI will train itself on new data on a set time cycle. After training, the AI will be tested against a comprehensive set of unit tests. If it passes the tests, the AI with the updated weight will be deployed. However if it fails, the AI will revert back to its previously trained weights and a report of the data with the unit tests it failed will be sent to the developers. This will all happen at a central location within the company that developed the AI. Every instance of the AI will be the same after each automated maintenance cycle. Pass and fail of the unit tests are subjective at this time.

			%The second question is how will the QA process be regulated to ensure the maintenance is keeping patients from being exposed to risk. This can be answered by using the premarket review of the AI. As part of what a company needs to submit to the FDA, they need to submit their tests. The FDA will set a standard that determines whether the unit tests are good enough for the automated maintenance of the AI. If the tests fail the premarket review, the AI returns to development until the next premarket review. On the other hand if the tests pass the review, the AI is allowed to be deployed where it will use the tests defined in the premarket review for its maintenance.

			%The tests however can not be changed once the AI is deployed after the premarket review. If the company wants to update its test based development done from failed unit testing, another premarket review will need to occur to define the tests again.

			%What it means to pass the premarket review can also change based on political and outside influences. Depending on what ideology controls the government can drastically change how hard or easy it is to pass the premarket review. One side is more of the spectrum is more inclined to push for a less restricted market. The other side is going to push for tighter regulations. In either case, there needs to be a balance between free market and regulation. Without enough free market, a few companies could control the majority of medical AI's. On the flip side with no regulation, patients' well-being comes to be at risk. The balance needs to be such that there is competition, but without risk to the patients. Lobbyists can also affect what goes into passing the premarket review. Initially, it will be companies sending these lobbyists likely to push for less regulations. However, eventually some type of activist group will show up to lobby for tighter regulations if medical AI's have become risky.

			%Figure 1: Framework for Automated Maintenance of a Medical AI \textbf{(Port graph to LaTeX at some point --Max)}

			%There are still problems and things to consider. It is hard to define what is "good enough" for these unit tests. In testing, edge cases are difficult to identify in something as complex as a medical AI. The performance of the AI might go up for what the unit tests are checking for, but could get incrementally worse for sometiming not check. Another thing to consider is political influence. Depending on the politics at the time, what is "good enough" could be lowered to the point of risking patients' health.
		
			%Overall, automated maintenance of a medical AI is feasible. There are many variables to consider when creating a framework that allows the AI to continuously learn without causing risk to patients. 

		%\subsection{Balance of Regulation Against the Free Market}\label{market}
			% This section will go into detail on the pros and cons of high and low regulation, emphasizing both the massive increase in risk that comes with low regulations and how high regulations could strangle innovation.

			
\medskip

\bibliographystyle{unsrt}
\bibliography{thompson,ternent}
		
\end{document}

