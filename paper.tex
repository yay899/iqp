\documentclass[]{article}

%opening
\title{We kind of need a title}
\author{
  Ternent, James\\
  \texttt{jwternent@wpi.edu}
  \and
  Thompson, Maximilian\\
  \texttt{mthompson2@wpi.edu}
}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Placeholder abstract
	\end{abstract}
	
	\section{Introduction}
		% This section will contain our research questions, and what we did to answer them (research, surveys, data analysis. Prefacing all of this will be the definition of some important terms that are used throughout the paper. Most of our research questions are outlined in another document.	
	
		Artificial intelligence (AI) and, more importantly, machine learning (ML) is becoming increasingly prevalent in medical fields. Though regulations are being discussed regarding AI as a whole, the regulations proposed by the FDA\cite{fdaregproposal} place a large emphasis on procedural AI which severely limits the capabilities of ML models. Simply refusing to implement those restrictions would cause more harm than good, however. We wish to add to this discussion in the hopes of creating both safer and more favorable conditions for medical ML models.

		Machine learning comprises a subset of AI that closely matches what most commonly comes to mind when AI is mentioned---both conversationally and within popular culture.\textbf{[Citation needed]} The difference between procedural AI and ML is detailed in section \ref{introtoai}, but, in essence, procedural AI takes input data and outputs a response by following a strict set of rules or procedures, while ML takes input data and outputs a response determined by a set of facets of interest and weights than may change and shift with each input. Frequently ML models undergo a period of "training", during which weights may shift wildly, followed by a period of relatively little change.\textbf{[Citation needed]}

		We also provide an overview of ethical considerations within the context of medical devices in section \ref{ethics} before continuing to weigh the pros and cons of machine learning. We take survey of a number of nursing students to gauge their general opinion, understanding, and expectations of ML. We believe, due to how overworked nurses can be\cite{doi:10.1111/j.1365-2648.2009.05082.x} and how much of an impact ML has had and will have on the nursing population\cite{Ham2017}, that their viewpoints are and important part of the conversation which we wish to highlight. Finally we delve into the role both regulation and the free market can play in this developing field, and attempt to strike a balance between the two that we hope could create a safe environment for patients and practitioners alike without stifling the development of more refined machine learning and artificial intelligence.

		% This part is definitely really rough, and needs to be expanded. It should potentially be moved to other situations. Probably to the intro to AI section.
		First however, we must define some terms used commonly in the ML field that may be misleading to those without a background in it. There are a number of terms used throughout the field of AI that may be misleading at first glance: "feature" refers to what amounts to an interesting property or value in the input data represented by a function explicitly defined by the developers of the AI. Within the context of ML, these functions are what is modified by changes in the weights of the model.\textbf{[Citation needed?]} Additionally, the term area under the curve (AUC) is used to express the quality of predictions derived from values referred to as sensitivity, the rate of positive predictions (true or false), and selectivity, the rate of negative predictions (true or false). It is important to note that the terms positive and negative apply to predictions, while true and false apply exclusively to the reality of the situation.\cite{introtoauc}

		Explainability of ML models is an important ongoing research field. An explanation of a model seeks to convey why a model made the decisions it did and why it outputted what it did, and explanations have two key attributes: completeness (how accurately the explanation conveys the workings of the ML model), and interpretability (how easy it is for humans to understand the explanation). The two are often at odds, and finding the solution to this problem is an important field of study\cite{8631448}---especially regarding the validation of ML model outputs.\cite{10.1145/3328519.3329126} Unfortunately, the term interpretability is not only applied to explanations, but also to the ML models themselves. When used to describe a model, the term refers to how easy it is to identify and understand the mechanisms that drive the machine. Essentially interpretability seeks to answer how the machine works, while explainability regards why the machine did what it did.\cite{8631448} More in depth definitions of these concepts are given in section \ref{introtoai}.

		This topic also leads to a number of ethical considerations (further discussed in section \ref{ethics}) as pointed out by Gilpin \emph{et al.} who state that "[they] believe that it is fundamentally unethical to present a simplified description of a complex system in order to increase trust if the limitations of the simplified description cannot be understood by users". They point out that an explanation that meets that criteria are both misleading and dangerous.\cite{8631448} Though they do not mention it, we would like to add that though it may often be unintentional, this could easily be used to intentionally deceive. 
		
	\section{Basics of Artificial Intelligence}\label{introtoai}
		Artificial intelligence (AI) in its basic form is a computer program that takes in an input or inputs and produces an output. An example of a procedural AI would be a program that took in how many hours it had been since someone showered and determined whether they were due for a shower or not. The AI would have a threshold that would trigger a recommendation to shower. For the purpose of this example we will set it at twenty-four hours. If it had been sixteen hours since someone showered, it would not tell them to shower, but if it had been twenty-seven hours since they showered, the AI would recommend a shower. However, we know time is not the only factor in whether someone should shower. To better improve this theoretical AI, a second input could be added such as whether the person had showered since their last shower. In general, people should shower after they exercise. The updated logic in the AI is that it will recommend a shower if the time since the last shower is greater than twenty-four hours or if they have exercised since their last shower. Only one condition has to be true to recommend a shower, but both could be true. The more inputs or parameters that the AI takes in the better it will get a giving a correct output as long the researcher determined the correct thresholds for the AI.

		Eventually as the AI gets more complex, two problems present themselves. First, not all problems that an AI tries to solve has a simple linear relation between their inputs and the output. Second, the researcher may not know the exact relation between the inputs and the output. This is where a subset of AI called Machine Learning (ML) comes into play. Neural networking, a method of ML, crudely simulates a brain. The way the brain works is it has many neurons that are connected together by synapses.

		% Machine learning (ML).

		% We really need to work hard to make it clear that we are discussing primarily ML models and not procedural AI. I've basically replaced AI with ML model in almost every part I've been writing below.

		% We need to detail the difference between supervised and unsupervised training.
		

	\section{Ethics of Medical Devices and ML}\label{ethics}
		% This section will go into the basic definition of ethics before going into more detail about how ethics apply to medical devices. There will also be some stuff on the legal and ethical considerations of using electronic health records as training data for learning AI.

	\section{Expectations of Medical AI}
		% This section will go into how nurses and patients view medical AI, as well as how they expect it to function. This will make heavy use of gathered survey data and highlight both differences between the two groups, and just general areas that could cause some concern if they are not kept in mind while developing medical applications of machine learning.

		A number of surveys summarized by Vayena \emph{et al.} show a clear divide between popular and professional opinion on medical ML: "63\% of the adult population [of the United Kingdom] is uncomfortable with allowing personal data to be used to improve healthcare and is unfavorable to artificial intelligence (AI) systems replacing doctors and nurses in tasks they usually perform." However, drawing from a German survey, they express that a large portion of medical students believe wholeheartedly that machine learning will improve medicine. Though they are not eager enough to embrace it with little caution, expressing that they are "skeptical that it will establish conclusive diagnoses." This opinion is roughly mirrored by another United States survey cited by the authors of the article.\cite{Vayena2018}

		We chose to conduct our own survey of nursing students to gather further opinions on the subject. As a frequently understaffed and overworked profession\cite{doi:10.1111/j.1365-2648.2009.05082.x}, we believe that among healthcare professionals they stand to gain the most from advances in medical ML. Our survey ran from... \textbf{(Fill this in later! --Max)}

	\section{Benefits of Medical AI}
		% This section will go into what sort of benefits medical AI may be able to provide regardless of what people expect of it. It will bring up how it can lighten the workload of overworked healthcare professionals, and how it may lead to better treatment selection by pulling from data a human would not expect to be relevant.

		When discussing the potential benefits of machine learning for use in medical settings, it's easy to assume that use cases will be centered around aiding health care professionals in decision making and diagnosis or patients in seeking the correct medical care, but there is just as much potential for machine learning to alleviate some of the stress of day to day nursing work simply by streamlining existing systems in ways that match any given situation. In 2016, Ham \emph{et al.} proposed and developed a proof of concept application that provides contextual assistance using wi-fi based location data to aid a machine learning algorithm.\cite{Ham2017}

	\section{Risks of Medical AI}
		Medical ML models carry with them great risk, the greatest of which stems from a popular misunderstanding of the risk associated with ML as a whole. Within popular media, the risk of ML is often centered entirely around the threat of some kind of AI uprising when the real threat is the lack of explainability and verifiability in most ML models along with the potential socio-economic and environmental impact of increased use of ML.\cite{bbc2016rroai,emerj2019roawrtiwwa}

		More often than not ML models are a black box that takes in datasets and outputs values with some prescribed significance. It is difficult for non-ML experts understand the risks and potential failure points of a given ML model, and methods of addressing this lack of interpretability of the model or explainability of the results are still being debated.\cite{10.1145/2858036.2858529, 10.1145/3328519.3329126} This in conjunction with a typical person's initial trust placing little to no emphasis on the actual functionality of a ML model\cite{siau2018building} can make for sudden and unexpected tragedy within medical fields. For example a CEHC study in the mid 90's found a rule-based trained to identify risk in pneumonia patients incorrectly identified patients with asthma as having lower risk due to the lower mortality rate following urgent movement to intensive care.\cite{caruana2015intelligible}

		There is immense difficulty of validating the predictions (before acting on them) of a ML model. This creates further risk by obfuscating potential error behind a highly technical veil for non-ML experts. Moreover, unknown error introduced in input datasets is difficult to detect without access to the data itself, as the ML model will continue to output seemingly valid predictions\cite{10.1145/3328519.3329126} Error could be introduced through any number of innocuous vectors (differences in storage systems between health centers, errors digitizing physical records, decay of electronic records over time) or through more malignant ones (such as a targeted attack). The delicacy of handling electronic health records can make it legally difficult to inspect them for these errors\textbf{[Citation needed]}, and the sheer size of the task is prohibitive.

		"Learning to Validate the Predictions of Black Box Machine Learning Models on Unseen Data"\cite{10.1145/3328519.3329126} provides method of detecting dataset shifts without directly examining the dataset itself. \textbf{(I don't entirely understand all the math here, so I'll write this later. Alternatively, James could handle this. --Max)}

		Not only can shifts in datasets cause unexpected error in outputs, but so can misapplication of a ML model and unintended bias introduced by the training datasets.

		\textbf{(Integrate what follows into the text more, and add citations --Max)}

		The maintenance of a medical artificial intelligence (AI) is an important aspect that needs to be thoroughly examined. Without the implementation of maintenance of the medical AI, the performance could be degraded as the AI is trained on new data. As time goes along, it would become more likely for some form of AI malpractice to happen.
	
		Maintenance could be manual or automated. However, manual maintenance runs into the problem of the extremely high workload, which makes it infeasible for humans to labor at. The only option is automated maintenance, but there are a variety of questions to consider to ensure a safe AI.
	
		The first of the questions is how will quality assurance (QA) be performed with an AI continuously learning. The initial proposed framework for QA is the AI will train itself on new data on a set time cycle. After training, the AI will be tested against a comprehensive set of unit tests. If it passes the tests, the AI with the updated weight will be deployed. However if it fails, the AI will revert back to its previously trained weights and a report of the data with the unit tests it failed will be sent to the developers. This will all happen at a central location within the company that developed the AI. Every instance of the AI will be the same after each automated maintenance cycle. Pass and fail of the unit tests are subjective at this time.

		The second question is how will the QA process be regulated to ensure the maintenance is keeping patients from being exposed to risk. This can be answered by using the premarket review of the AI. As part of what a company needs to submit to the FDA, they need to submit their tests. The FDA will set a standard that determines whether the unit tests are good enough for the automated maintenance of the AI. If the tests fail the premarket review, the AI returns to development until the next premarket review. On the other hand if the tests pass the review, the AI is allowed to be deployed where it will use the tests defined in the premarket review for its maintenance.

		The tests however can not be changed once the AI is deployed after the premarket review. If the company wants to update its test based development done from failed unit testing, another premarket review will need to occur to define the tests again.

		What it means to pass the premarket review can also change based on political and outside influences. Depending on what ideology controls the government can drastically change how hard or easy it is to pass the premarket review. One side is more of the spectrum is more inclined to push for a less restricted market. The other side is going to push for tighter regulations. In either case, there needs to be a balance between free market and regulation. Without enough free market, a few companies could control the majority of medical AI’s. On the flip side with no regulation, patients’ well-being comes to be at risk. The balance needs to be such that there is competition, but without risk to the patients. Lobbyists can also affect what goes into passing the premarket review. Initially, it will be companies sending these lobbyists likely to push for less regulations. However, eventually some type of activist group will show up to lobby for tighter regulations if medical AI’s have become risky.

		Figure 1: Framework for Automated Maintenance of a Medical AI \textbf{(Port graph to LaTeX at some point --Max)}

		There are still problems and things to consider. It is hard to define what is “good enough” for these unit tests. In testing, edge cases are difficult to identify in something as complex as a medical AI. The performance of the AI might go up for what the unit tests are checking for, but could get incrementally worse for sometiming not check. Another thing to consider is political influence. Depending on the politics at the time, what is “good enough” could be lowered to the point of risking patients’ health.
	
		Overall, automated maintenance of a medical AI is feasible. There are many variables to consider when creating a framework that allows the AI to continuously learn without causing risk to patients. 

	\subsection{Balance of Regulation Against the Free Market}
		% This section will go into detail on the pros and cons of high and low regulation, emphasizing both the massive increase in risk that comes with low regulations and how high regulations could strangle innovation.

		

\medskip

\bibliographystyle{unsrt}
\bibliography{thompson,ternent}
		
\end{document}

